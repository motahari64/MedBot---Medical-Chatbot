{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMkVdZIpxa3Vf6wUikiLcLN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/motahari64/MedBot---Medical-Chatbot/blob/main/Medbot_main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain openai tiktoken unstructured chromadb"
      ],
      "metadata": {
        "id": "boHrizMICt-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "#Specify your API key in OPENAI\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\""
      ],
      "metadata": {
        "id": "ueDxvwweCuBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "RLJTCmB8CuEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Build a vectorDB**"
      ],
      "metadata": {
        "id": "0QcW7Fc0C3Dc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a sample vectorDB\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "#Specify the path to the folder containing the TXT files\n",
        "path = ''\n",
        "loader = DirectoryLoader(path, glob=\"**/*.txt\")\n",
        "data=loader.load()\n",
        "\n",
        "# Split\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
        "splits = text_splitter.split_documents(data)\n",
        "\n",
        "# VectorDB\n",
        "embedding = OpenAIEmbeddings()\n",
        "vectordb = Chroma.from_documents(documents=splits, embedding=embedding)\n",
        "len(data)"
      ],
      "metadata": {
        "id": "OitJEnqfCuJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectordb.as_retriever()"
      ],
      "metadata": {
        "id": "_1jITKYUCuG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Now we need to create a tool for our retriever. The main things we need to pass in are a name for the retriever as well as a description.\n",
        "## These will both be used by the language model, so they should be informative.\n",
        "\n",
        "from langchain.agents.agent_toolkits import create_retriever_tool\n",
        "\n",
        "tool = create_retriever_tool(\n",
        "    retriever,\n",
        "   \"search_documents\",\n",
        "    \"Searches and returns documents regarding the txt file in resource folder\",\n",
        ")\n",
        "\n",
        "tools = [tool]"
      ],
      "metadata": {
        "id": "Jckqi713CuME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question_input = input(\"what is your question ? \")"
      ],
      "metadata": {
        "id": "gDMKIRX3CuOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import create_extraction_chain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# Schema\n",
        "schema = {\n",
        "    \"properties\": {\n",
        "        \"sickness\": {\"type\": \"string\"},\n",
        "        \"drug\": {\"type\": \"string\"},\n",
        "        \"symptoms\": {\"type\": \"string\"},\n",
        "        \"symptoms_organ\": {\"type\": \"string\"},\n",
        "        \"symptoms_extra_info\": {\"type\": \"string\"}\n",
        "    },\n",
        "    \"required\": [\"sickness\",\"drug\",\"symptoms\"],\n",
        "}\n",
        "\n",
        "# Input\n",
        "inp = question_input\n",
        "# Run chain\n",
        "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
        "chain = create_extraction_chain(schema, llm)\n",
        "\n",
        "extract=chain.run(inp)\n",
        "\n",
        "\n",
        "chain.run(inp)         ##show all\n",
        "\n"
      ],
      "metadata": {
        "id": "cSskcIIiCuRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Agent Constructor**"
      ],
      "metadata": {
        "id": "wjSRGAbXDJBT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Here, we will use the high level create_conversational_retrieval_agent API to construct the agent.\n",
        "\n",
        "from langchain.agents.agent_toolkits import create_conversational_retrieval_agent\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(temperature=0)"
      ],
      "metadata": {
        "id": "9rb1IX1eDKis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "\n",
        "\n",
        "prompt_template = PromptTemplate.from_template(\n",
        "    \"answer the question this : {question} by search only available resource text. Extract the answer to the question '{question}' or say 'not found' if the information is not contained.\"\n",
        ")\n",
        "if(extract):\n",
        "  agent_executor = create_conversational_retrieval_agent(llm, tools, verbose=True)\n",
        "  result = agent_executor(\n",
        "    {\n",
        "        \"input\":prompt_template.format(question=question_input)\n",
        "    }\n",
        "  )\n",
        "else : print(\"I am medical assistant, please ask your medical question only\")\n",
        "\n",
        "result[\"output\"]\n"
      ],
      "metadata": {
        "id": "mBRYkbR4DKqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TIBl0-KGDKx8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}